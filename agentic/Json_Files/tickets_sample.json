[
  {
    "ticket_id": "TKT-0001",
    "title": "ADF Linked Service to Azure SQL failing with ErrorCode 2108 (Key Vault secret not found)",
    "description": "Azure Data Factory pipeline runs fail when using a Linked Service to Azure SQL. ErrorCode=2108 indicates the referenced Key Vault secret version does not exist. Failures started after rotating secrets yesterday.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "High",
    "application": "ADF - Linked Service: Azure SQL (prod)",
    "affected_users": "All nightly ingestion pipelines depending on Azure SQL connection",
    "environment": "Production",
    "solution": "1. Updated Linked Service to use 'latest' secret version in Key Vault.\n2. Synced ADF managed identity permissions on Key Vault (Get/List).\n3. Triggered pipeline reruns and validated successful connectivity.",
    "reasoning": "Secret rotation created a new version but Linked Service pinned an old version, leading to 2108 errors.",
    "timestamp": "2025-11-11 08:30:00"
  },
  {
    "ticket_id": "TKT-0002",
    "title": "ADF Self-hosted IR offline causing copy activity timeouts",
    "description": "Copy activities to on-prem SQL Server fail with timeout; Self-hosted Integration Runtime shows 'offline' after Windows patching.",
    "category": "Azure Data Factory",
    "status": "In Progress",
    "severity": "Critical",
    "application": "ADF - Self-Hosted IR",
    "affected_users": "Finance and sales data loads from on-prem",
    "environment": "Production",
    "solution": "1. Restarted IR service and validated node status.\n2. Opened firewall ports 8060-8061 and checked outbound endpoints.\n3. Scheduled HA node to avoid single-node outages.",
    "reasoning": "Windows update reset firewall rules; IR lost connectivity to ADF service endpoints.",
    "timestamp": "2025-11-11 09:10:00"
  },
  {
    "ticket_id": "TKT-0003",
    "title": "ADF Copy Activity schema drift failing with ColumnNotFound",
    "description": "Source table added a new nullable column; sink mapping in ADF dataset is fixed. Runs fail with 'Column not found in source mapping'.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "Medium",
    "application": "ADF - Copy Pipeline: CustomerDim",
    "affected_users": "BI consumers of dimensional model",
    "environment": "Staging",
    "solution": "1. Enabled auto-mapping and 'preserveAdditionalColumns' in mapping.\n2. Regenerated dataset schema from source.\n3. Added schema compatibility check in pre-deploy validation.",
    "reasoning": "Manual column mapping drifted from upstream schema changes.",
    "timestamp": "2025-11-11 10:05:00"
  },
  {
    "ticket_id": "TKT-0004",
    "title": "ADF Private Endpoint to Storage failing with 403 AuthorizationFailure",
    "description": "Pipelines writing to ADLS Gen2 via private endpoint fail with 403. Storage firewall blocks public network; ADF managed VNET not configured for this storage account.",
    "category": "Azure Data Factory",
    "status": "In Progress",
    "severity": "High",
    "application": "ADF - Managed VNET",
    "affected_users": "Data lake ingestion workloads",
    "environment": "Production",
    "solution": "1. Created Private Endpoint in the storage account's VNET.\n2. Approved PE connection and updated DNS zones for blob/dfs endpoints.\n3. Switched Linked Service to 'Managed Private Endpoint' and revalidated.",
    "reasoning": "Storage firewall forced private access; missing managed private endpoint configuration.",
    "timestamp": "2025-11-11 11:20:00"
  },
  {
    "ticket_id": "TKT-0005",
    "title": "ADF Trigger misfire due to time zone misconfiguration",
    "description": "Schedule trigger expected at 02:00 local time but executed at 02:00 UTC, breaking SLA for downstream jobs.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "Low",
    "application": "ADF - Trigger: NightlyBatch",
    "affected_users": "Downstream consumers expecting 06:30 reports",
    "environment": "Production",
    "solution": "1. Updated trigger time zone to account for DST.\n2. Added run window guard to prevent off-window runs.\n3. Implemented post-deploy validation of trigger local-time alignment.",
    "reasoning": "Trigger used UTC while the business assumption was local time with DST.",
    "timestamp": "2025-11-11 12:10:00"
  },
  {
    "ticket_id": "TKT-0006",
    "title": "ADF ForEach + Lookup pagination failing with 429 throttling",
    "description": "API source throttles when ADF parallel ForEach hits endpoint; pipeline fails intermittently with 429 and partial loads.",
    "category": "Azure Data Factory",
    "status": "In Progress",
    "severity": "Medium",
    "application": "ADF - REST Connector",
    "affected_users": "Data engineering team and API-based datasets",
    "environment": "Production",
    "solution": "1. Added sequential mode with concurrency=2 and exponential retry policy.\n2. Implemented server-side cursor pagination with 'nextLink'.\n3. Cached tokens to reduce auth overhead.",
    "reasoning": "Excessive parallelism triggered vendor API rate limits.",
    "timestamp": "2025-11-11 13:00:00"
  },
  {
    "ticket_id": "TKT-0007",
    "title": "ADF Copy to Synapse failing with PolyBase CI policy restrictions",
    "description": "Copy via staging storage fails with PolyBase external table creation due to restricted CREATE EXTERNAL TABLE permissions.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "High",
    "application": "ADF - Synapse Sink",
    "affected_users": "Analytics warehouse team",
    "environment": "Production",
    "solution": "1. Switched to 'Copy command' instead of PolyBase.\n2. Granted minimal required permissions via RBAC change request.\n3. Documented sink option selection matrix per environment.",
    "reasoning": "Enterprise security baseline blocked PolyBase DDL in prod.",
    "timestamp": "2025-11-11 13:45:00"
  },
  {
    "ticket_id": "TKT-0008",
    "title": "ADF Data Flow fails with 'Out of memory' on large joins",
    "description": "Mapping Data Flow job aborts with OOM when joining two 200M-row datasets; current core count insufficient.",
    "category": "Azure Data Factory",
    "status": "In Progress",
    "severity": "High",
    "application": "ADF - Mapping Data Flow",
    "affected_users": "Data modelers and monthly reconciliations",
    "environment": "Production",
    "solution": "1. Increased DIUs and enabled broadcast join where safe.\n2. Added pre-aggregation and partition pruning on join keys.\n3. Scheduled job during low cluster utilization window.",
    "reasoning": "Insufficient compute for skewed join; broadcast and pruning reduce shuffle and memory pressure.",
    "timestamp": "2025-11-11 14:30:00"
  },
  {
    "ticket_id": "TKT-0009",
    "title": "ADF Linked Service to SFTP failing host key validation",
    "description": "After vendor rotated host keys, SFTP connected but failed strict host key check; copy activities aborted.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "Medium",
    "application": "ADF - SFTP Connector",
    "affected_users": "External vendor data imports",
    "environment": "Production",
    "solution": "1. Updated host key fingerprint in Linked Service.\n2. Implemented vendor rotation notification into runbook.\n3. Added validation test step pre-run.",
    "reasoning": "Host key fingerprint mismatch after vendor rotation.",
    "timestamp": "2025-11-11 15:20:00"
  },
  {
    "ticket_id": "TKT-0010",
    "title": "ADF Key Vault access denied for MSI after subscription move",
    "description": "ADF managed identity lost Key Vault access post-subscription migration; all secret-backed linked services failing.",
    "category": "Azure Data Factory",
    "status": "Resolved",
    "severity": "Critical",
    "application": "ADF - Managed Identity",
    "affected_users": "All pipelines using KV-based secrets",
    "environment": "Production",
    "solution": "1. Re-assigned Key Vault access policies/RBAC for ADF MSI.\n2. Validated list/get permissions and secret resolution.\n3. Added post-migration checklist for identity rebindings.",
    "reasoning": "Subscription move detached RBAC bindings for managed identity.",
    "timestamp": "2025-11-11 16:05:00"
  },
  {
    "ticket_id": "TKT-0011",
    "title": "Databricks cluster cannot reach Azure SQL (TCP handshake failure)",
    "description": "JDBC connections from Databricks to Azure SQL intermittently fail: 'The TCP/IP connection to the host ... failed' with error 11001. Occurs during peak hours.",
    "category": "Databricks",
    "status": "In Progress",
    "severity": "High",
    "application": "Databricks Workspace - JDBC",
    "affected_users": "ETL jobs and ad-hoc analysts",
    "environment": "Production",
    "solution": "1. Validated VNet peering and NSGs; opened outbound 1433.\n2. Enabled retry with exponential backoff in JDBC options.\n3. Moved to Private Link for stable connectivity.",
    "reasoning": "Intermittent network path instability; missing Private Link increased exposure to transient failures.",
    "timestamp": "2025-11-11 17:00:00"
  },
  {
    "ticket_id": "TKT-0012",
    "title": "Databricks DBFS mount to ADLS broken after SPN secret rotation",
    "description": "dbutils.fs.mount paths return 403; service principal secret expired. Notebooks that rely on mounted paths fail.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "High",
    "application": "Databricks - DBFS Mounts",
    "affected_users": "All teams reading from /mnt/raw",
    "environment": "Production",
    "solution": "1. Rotated SPN secret and updated Key Vault-backed scope.\n2. Remounted DBFS paths with new creds and validated access.\n3. Added rotation runbook and alert for secret expiry.",
    "reasoning": "Expired service principal secret for storage access caused 403s.",
    "timestamp": "2025-11-11 17:40:00"
  },
  {
    "ticket_id": "TKT-0013",
    "title": "Databricks workspace DNS resolution failing for private endpoints",
    "description": "Clusters cannot resolve privatelink database endpoints; custom DNS zone not linked to VNet used by clusters.",
    "category": "Databricks",
    "status": "In Progress",
    "severity": "Medium",
    "application": "Databricks - Networking",
    "affected_users": "Jobs using private endpoints (SQL, Storage)",
    "environment": "Production",
    "solution": "1. Linked Private DNS zones to cluster VNet and validated A records.\n2. Flushed DNS cache at node level.\n3. Added Terraform validation for zone links.",
    "reasoning": "Missing DNS zone link prevented resolution of privatelink FQDNs.",
    "timestamp": "2025-11-11 18:25:00"
  },
  {
    "ticket_id": "TKT-0014",
    "title": "Databricks library installation fails due to wheel resolver conflict",
    "description": "Notebook pip install of pyspark + mlflow combo fails with resolver conflicts; cluster init script pins incompatible versions.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Databricks - Libraries",
    "affected_users": "Data science notebooks",
    "environment": "Staging",
    "solution": "1. Moved dependencies to cluster-level init script with locked versions.\n2. Created requirements.txt + index mirror for reproducibility.\n3. Added preflight dependency compatibility check.",
    "reasoning": "Ad-hoc installs conflicted with cluster init dependencies.",
    "timestamp": "2025-11-11 19:00:00"
  },
  {
    "ticket_id": "TKT-0015",
    "title": "Databricks job aborted: SparkException due to skewed shuffle",
    "description": "ETL job fails with 'Job aborted due to stage failure' and excessive spill; single partition holds 60% of data.",
    "category": "Databricks",
    "status": "In Progress",
    "severity": "High",
    "application": "Databricks - Spark Jobs",
    "affected_users": "Downstream reporting feeds",
    "environment": "Production",
    "solution": "1. Enabled skew join hints and salting for hot keys.\n2. Increased shuffle partitions and autoscaling max workers.\n3. Added metrics on partition size distribution.",
    "reasoning": "Key skew leading to hotspot partitions and spills to disk.",
    "timestamp": "2025-11-11 19:45:00"
  },
  {
    "ticket_id": "TKT-0016",
    "title": "Databricks token expired causing REST jobs API failures",
    "description": "Automation using Jobs API returns 401; PAT expired over weekend, no rotation in place.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "Low",
    "application": "Databricks - Automation",
    "affected_users": "Platform automation pipelines",
    "environment": "Production",
    "solution": "1. Generated new PAT with least privilege and rotated CI secrets.\n2. Added expiry alert and automated rotation process.\n3. Migrated to service principal auth where possible.",
    "reasoning": "Human PAT used for automation without rotation policy.",
    "timestamp": "2025-11-11 20:15:00"
  },
  {
    "ticket_id": "TKT-0017",
    "title": "Databricks to Synapse connector failing with TLS handshake error",
    "description": "JDBC to Synapse fails 'handshake_failure'; cluster uses outdated JVM cipher suite policy.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Databricks - Synapse Connector",
    "affected_users": "Warehouse ELT pipelines",
    "environment": "Staging",
    "solution": "1. Upgraded cluster runtime to latest LTS.\n2. Enabled compatible TLS1.2 cipher suites per Synapse recommendations.\n3. Added pre-connection TLS sanity check.",
    "reasoning": "Incompatible cipher suites in older runtime caused TLS handshake failures.",
    "timestamp": "2025-11-11 20:50:00"
  },
  {
    "ticket_id": "TKT-0018",
    "title": "Databricks Unity Catalog permission denied on table write",
    "description": "Delta write fails with 'PERMISSION_DENIED' after table moved under UC; service principal lacks 'WRITE' on schema.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "High",
    "application": "Databricks - Unity Catalog",
    "affected_users": "ETL owners writing curated tables",
    "environment": "Production",
    "solution": "1. Granted USAGE on catalog + schema and WRITE on target table to SPN.\n2. Updated ACL management via Terraform.\n3. Added permission checks in pre-deploy.",
    "reasoning": "UC migration changed auth model; missing grants blocked writes.",
    "timestamp": "2025-11-11 21:25:00"
  },
  {
    "ticket_id": "TKT-0019",
    "title": "Databricks job cluster fails to start due to subnet IP exhaustion",
    "description": "Cluster startup stuck; error shows unable to allocate IP from subnet. Recent scale-out consumed remaining IPs.",
    "category": "Databricks",
    "status": "In Progress",
    "severity": "High",
    "application": "Databricks - Networking",
    "affected_users": "All scheduled job clusters in subnet-analytics",
    "environment": "Production",
    "solution": "1. Expanded subnet address space via planned change.\n2. Reduced idle clusters and enforced auto-termination.\n3. Capacity planning added to weekly ops review.",
    "reasoning": "Subnet exhausted IPs preventing new node allocation.",
    "timestamp": "2025-11-11 22:10:00"
  },
  {
    "ticket_id": "TKT-0020",
    "title": "Databricks Delta table vacuum removed needed files (data loss risk)",
    "description": "VACUUM ran with too-low retention (8h) while streaming job lagged; queries return 'File not found'.",
    "category": "Databricks",
    "status": "Resolved",
    "severity": "Critical",
    "application": "Databricks - Delta Lake",
    "affected_users": "Streaming + BI queries on bronze layer",
    "environment": "Production",
    "solution": "1. Restored from snapshots and increased retention to 168h.\n2. Enabled 'vacuum retention check' and blocked unsafe settings.\n3. Monitored streaming lag before housekeeping tasks.",
    "reasoning": "Aggressive vacuum deleted files still referenced by lagging streaming readers.",
    "timestamp": "2025-11-11 23:00:00"
  },
  {
    "ticket_id": "TKT-0021",
    "title": "Notebook error: ModuleNotFoundError after runtime upgrade",
    "description": "After upgrading to new cluster runtime, notebooks fail importing company-internal package; wheel not installed via init scripts.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Analytics Notebooks",
    "affected_users": "Data scientists working on forecasting",
    "environment": "Staging",
    "solution": "1. Added wheel to cluster init script and tested import.\n2. Pinned runtime for project until compatibility suite passes.\n3. Added smoke test notebook in CI.",
    "reasoning": "Runtime upgrade dropped implicit dependency; missing init script install.",
    "timestamp": "2025-11-12 00:20:00"
  },
  {
    "ticket_id": "TKT-0022",
    "title": "Notebook fails on pandas read_parquet due to Arrow version mismatch",
    "description": "read_parquet throws ArrowInvalid; mixing parquet files written by different versions; schema evolution not handled.",
    "category": "Notebooks",
    "status": "In Progress",
    "severity": "Medium",
    "application": "Exploratory Notebooks",
    "affected_users": "BI analysts in ad-hoc analysis",
    "environment": "Development",
    "solution": "1. Standardized writer options and upgraded pyarrow across environments.\n2. Added schema reconciliation step before read.\n3. Wrote compatibility test for parquet versions.",
    "reasoning": "Heterogeneous parquet metadata caused read failures.",
    "timestamp": "2025-11-12 01:00:00"
  },
  {
    "ticket_id": "TKT-0023",
    "title": "Notebook timeout during large matplotlib render in headless mode",
    "description": "Export to PNG of 60k-point charts times out; backend agg not optimized; memory spikes on driver.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Low",
    "application": "Reporting Notebooks",
    "affected_users": "Insights publishing workflow",
    "environment": "Production",
    "solution": "1. Switched to datashader for large plots.\n2. Limited points via downsampling and added caching.\n3. Increased driver memory slightly and set non-interactive backend.",
    "reasoning": "Rendering huge datasets with matplotlib default backend is slow and memory heavy.",
    "timestamp": "2025-11-12 01:30:00"
  },
  {
    "ticket_id": "TKT-0024",
    "title": "Notebook fails: OSError when writing to /dbfs due to quota reached",
    "description": "Model artifact write fails with 'No space left on device' on DBFS root; quota exceeded by orphaned artifacts.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "High",
    "application": "Model Training Notebooks",
    "affected_users": "ML engineers pushing models",
    "environment": "Production",
    "solution": "1. Cleaned up orphaned runs and artifacts via automated retention.\n2. Moved large artifacts to external ADLS path.\n3. Added quota alerts and cleanup job.",
    "reasoning": "Unbounded artifact storage filled DBFS quota.",
    "timestamp": "2025-11-12 02:05:00"
  },
  {
    "ticket_id": "TKT-0025",
    "title": "Notebook credential passthrough failing with Azure AD token audience mismatch",
    "description": "Access token acquired for wrong resource; notebook cannot read from ADLS Gen2 with ABFS driver.",
    "category": "Notebooks",
    "status": "In Progress",
    "severity": "High",
    "application": "Secure Data Access Notebooks",
    "affected_users": "Analysts using pass-through authentication",
    "environment": "Production",
    "solution": "1. Corrected OAuth configs to request 'https://storage.azure.com/'.\n2. Validated cluster identity passthrough settings.\n3. Added pre-run token validation widget.",
    "reasoning": "Wrong token audience prevented storage authorization.",
    "timestamp": "2025-11-12 02:40:00"
  },
  {
    "ticket_id": "TKT-0026",
    "title": "Notebook Git integration broken after PAT revocation",
    "description": "Repos sync fails with 403; developer PAT revoked during org security cleanup.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Low",
    "application": "Notebook SCM",
    "affected_users": "Data scientists syncing notebooks",
    "environment": "Staging",
    "solution": "1. Migrated to organization-managed service principal auth.\n2. Rotated credentials in Databricks Repos settings.\n3. Implemented SSO-based access with short-lived tokens.",
    "reasoning": "User PAT reliance is fragile; move to managed identity.",
    "timestamp": "2025-11-12 03:15:00"
  },
  {
    "ticket_id": "TKT-0027",
    "title": "Notebook import fails due to UnicodeDecodeError on CSV",
    "description": "read_csv throws UnicodeDecodeError for vendor file with mixed encodings.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Data Prep Notebooks",
    "affected_users": "Ops analysts loading vendor files",
    "environment": "Development",
    "solution": "1. Enabled encoding detection with chardet and enforced UTF-8 output.\n2. Added validation step in pipeline to normalize encodings.\n3. Documented vendor file requirements.",
    "reasoning": "Mixed encodings caused parser to fail on certain bytes.",
    "timestamp": "2025-11-12 03:50:00"
  },
  {
    "ticket_id": "TKT-0028",
    "title": "Notebook job fails due to missing environment variables in job cluster",
    "description": "Notebook runs fine interactively but fails as a job; env vars not propagated to job cluster.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Scheduled Notebooks",
    "affected_users": "Ops automation scripts",
    "environment": "Production",
    "solution": "1. Moved secrets to Databricks Secret Scopes and referenced in job.\n2. Added job-level environment variables in task settings.\n3. Synced config via Terraform.",
    "reasoning": "Interactive cluster had env configured; job cluster started clean.",
    "timestamp": "2025-11-12 04:25:00"
  },
  {
    "ticket_id": "TKT-0029",
    "title": "Notebook fails with 'NoSuchKey' when reading S3-compatible lake",
    "description": "Path resolution wrong due to missing trailing slash in prefix; reads intermittently fail.",
    "category": "Notebooks",
    "status": "Resolved",
    "severity": "Low",
    "application": "Cloud Interop Notebooks",
    "affected_users": "Analysts reading external lake",
    "environment": "Staging",
    "solution": "1. Corrected prefix handling and added path normalization helper.\n2. Wrote integration test for object store path rules.\n3. Added retry with backoff for eventual consistency.",
    "reasoning": "Incorrect path concatenation led to missing key lookups.",
    "timestamp": "2025-11-12 05:00:00"
  },
  {
    "ticket_id": "TKT-0030",
    "title": "Notebook 'OutOfMemoryError' during XGBoost training",
    "description": "Model training on 50M rows fails on driver with OOM; using single-node training.",
    "category": "Notebooks",
    "status": "In Progress",
    "severity": "High",
    "application": "ML Training Notebooks",
    "affected_users": "Data science team",
    "environment": "Production",
    "solution": "1. Switched to distributed training and increased driver memory.\n2. Downcast dtypes and enabled sparse matrices.\n3. Implemented early stopping and chunked training.",
    "reasoning": "Single-node training exceeded driver memory; distribution and memory optimizations required.",
    "timestamp": "2025-11-12 05:40:00"
  },
  {
    "ticket_id": "TKT-0031",
    "title": "Logic App HTTP action returns 401 after connector token expiry",
    "description": "Logic App calling internal API fails with 401; managed identity not configured, using outdated connection reference.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "High",
    "application": "Logic App - OrderProcessor",
    "affected_users": "Order processing automation",
    "environment": "Production",
    "solution": "1. Migrated to managed identity auth for HTTP action.\n2. Rotated connection reference and validated access.\n3. Added token expiry alert and run-after policy.",
    "reasoning": "Expired OAuth token in connection caused repeated 401s; MI avoids token drift.",
    "timestamp": "2025-11-12 06:10:00"
  },
  {
    "ticket_id": "TKT-0032",
    "title": "Logic App trigger skipped due to concurrency control saturation",
    "description": "High event volume caused trigger to drop events; concurrency limit set to 1 with long-running actions.",
    "category": "Logic Apps",
    "status": "In Progress",
    "severity": "Medium",
    "application": "Logic App - InboundWebhook",
    "affected_users": "Partner integrations",
    "environment": "Production",
    "solution": "1. Increased trigger concurrency and enabled parallel branches.\n2. Implemented queuing using Service Bus.\n3. Added idempotency to downstream actions.",
    "reasoning": "Serial processing under bursty traffic led to dropped runs.",
    "timestamp": "2025-11-12 06:45:00"
  },
  {
    "ticket_id": "TKT-0033",
    "title": "Logic App 'Parse JSON' fails due to schema drift",
    "description": "Vendor payload added optional field; strict schema validation breaks and run halts.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Low",
    "application": "Logic App - VendorIngestion",
    "affected_users": "Ops team processing vendor events",
    "environment": "Staging",
    "solution": "1. Relaxed schema to allow additionalProperties and optional fields.\n2. Added pre-validation step with fallback defaults.\n3. Implemented versioned schema handling.",
    "reasoning": "Strict JSON schema incompatible with backward-compatible vendor change.",
    "timestamp": "2025-11-12 07:20:00"
  },
  {
    "ticket_id": "TKT-0034",
    "title": "Logic App 429s from SharePoint connector during batch",
    "description": "List item updates hit throttling windows; runs fail with 'Too Many Requests'.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Logic App - SharePointSync",
    "affected_users": "PMO document workflows",
    "environment": "Production",
    "solution": "1. Implemented retry policy with jitter and reduced batch size.\n2. Introduced time-window scheduling aligned with SPO limits.\n3. Added backoff logic via do-until.",
    "reasoning": "Connector throttling under high-volume batch updates.",
    "timestamp": "2025-11-12 08:00:00"
  },
  {
    "ticket_id": "TKT-0035",
    "title": "Logic App malformed JSON to Service Bus action",
    "description": "Action fails with 'BadRequest' when building SB message; missing contentType header and incorrect base64 encoding.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Low",
    "application": "Logic App - EventDispatcher",
    "affected_users": "Downstream event consumers",
    "environment": "Production",
    "solution": "1. Set contentType to application/json and used proper json() function.\n2. Validated payload with compose + schema check.\n3. Added integration test in dev.",
    "reasoning": "Incorrect message construction caused connector to reject payload.",
    "timestamp": "2025-11-12 08:35:00"
  },
  {
    "ticket_id": "TKT-0036",
    "title": "Logic App FTP action fails due to TLS downgrade on vendor server",
    "description": "Connector rejects TLS1.0 offered by vendor FTP; handshake fails intermittently.",
    "category": "Logic Apps",
    "status": "In Progress",
    "severity": "Medium",
    "application": "Logic App - VendorFTP",
    "affected_users": "Finance EDI transfers",
    "environment": "Production",
    "solution": "1. Coordinated vendor TLS upgrade to 1.2.\n2. Switched to SFTP connector temporarily.\n3. Enforced cipher policy in connection settings.",
    "reasoning": "Vendor's weak TLS configuration incompatible with connector security policy.",
    "timestamp": "2025-11-12 09:10:00"
  },
  {
    "ticket_id": "TKT-0037",
    "title": "Logic App run-after not configured, causing silent downstream skips",
    "description": "Failure in first action aborts flow; downstream actions not set to run-after 'failed' or 'skipped'.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Low",
    "application": "Logic App - AlertRouter",
    "affected_users": "Ops alerting workflows",
    "environment": "Staging",
    "solution": "1. Configured run-after with comprehensive failure paths.\n2. Added scoped try/catch with terminate action.\n3. Implemented dead-lettering to Service Bus for failed messages.",
    "reasoning": "Default run-after caused hidden skips; explicit failure handling required.",
    "timestamp": "2025-11-12 09:45:00"
  },
  {
    "ticket_id": "TKT-0038",
    "title": "Logic App 'Until' loop exceeds 5000 iterations causing timeout",
    "description": "Polling pattern used incorrect condition; runs exceed default duration limit and time out.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Logic App - Poller",
    "affected_users": "Ops monitoring automation",
    "environment": "Production",
    "solution": "1. Implemented exponential backoff and max attempts.\n2. Replaced polling with event-driven webhook.\n3. Added circuit breaker to stop runaway loops.",
    "reasoning": "Busy-wait loop configuration led to excessive iterations and timeouts.",
    "timestamp": "2025-11-12 10:20:00"
  },
  {
    "ticket_id": "TKT-0039",
    "title": "Logic App 'When a HTTP request is received' 413 Payload Too Large",
    "description": "Large webhook payloads rejected; integration expected up to 25MB documents, default size limits smaller.",
    "category": "Logic Apps",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Logic App - Intake",
    "affected_users": "External partners submitting documents",
    "environment": "Production",
    "solution": "1. Switched to Blob drop-off + Event Grid trigger for large payloads.\n2. Validated chunk upload approach.\n3. Documented limits for partners.",
    "reasoning": "HTTP trigger unsuitable for large payloads; use storage-based intake.",
    "timestamp": "2025-11-12 10:55:00"
  },
  {
    "ticket_id": "TKT-0040",
    "title": "Logic App Managed Connector shows intermittent 502 Bad Gateway",
    "description": "Office365 connector intermittently returns 502; platform incident ongoing in region.",
    "category": "Logic Apps",
    "status": "In Progress",
    "severity": "Low",
    "application": "Logic App - Notifications",
    "affected_users": "Notification workflows",
    "environment": "Production",
    "solution": "1. Implemented retry with timeouts and graceful degradation.\n2. Routed critical notifications via secondary channel.\n3. Tracking vendor RCA.",
    "reasoning": "Platform-side intermittent outage; retries and fallback mitigate impact.",
    "timestamp": "2025-11-12 11:30:00"
  },
  {
    "ticket_id": "TKT-0041",
    "title": "DB: Azure SQL connection pool exhaustion under spike",
    "description": "API layer returns 500s due to SqlException 'Timeout expired'. Connection pool maxed during batch job overlap.",
    "category": "Database",
    "status": "Resolved",
    "severity": "Critical",
    "application": "Azure SQL - AppDB",
    "affected_users": "External API consumers",
    "environment": "Production",
    "solution": "1. Increased pool size and enabled connection resiliency.\n2. Separated batch workload to a dedicated pool.\n3. Added query throttling during overlap window.",
    "reasoning": "Contended connections between API and batch overwhelmed pool.",
    "timestamp": "2025-11-12 12:05:00"
  },
  {
    "ticket_id": "TKT-0042",
    "title": "DB: Deadlocks detected on Orders table during index rebuild",
    "description": "Nightly index maintenance overlapped with write-heavy hour, causing deadlocks and transaction rollbacks.",
    "category": "Database",
    "status": "Resolved",
    "severity": "High",
    "application": "Azure SQL - Orders",
    "affected_users": "Checkout service",
    "environment": "Production",
    "solution": "1. Rescheduled index rebuild to off-peak.\n2. Used ONLINE=ON and reduced lock footprint.\n3. Tuned long-running transactions.",
    "reasoning": "Maintenance operations conflicted with OLTP workload.",
    "timestamp": "2025-11-12 12:40:00"
  },
  {
    "ticket_id": "TKT-0043",
    "title": "DB: High DTU consumption due to missing indexes on search endpoints",
    "description": "p95 latency increased; query scans large table; missing composite index on (tenantId, createdAt).",
    "category": "Database",
    "status": "Resolved",
    "severity": "High",
    "application": "Azure SQL - Search",
    "affected_users": "Customers using search feature",
    "environment": "Production",
    "solution": "1. Added composite index and verified plan.\n2. Cached common queries at API gateway.\n3. Implemented query hints to prevent regressions.",
    "reasoning": "Table scan due to missing index caused high DTU and latency.",
    "timestamp": "2025-11-12 13:15:00"
  },
  {
    "ticket_id": "TKT-0044",
    "title": "DB: Transaction log near full; backups delayed",
    "description": "Log backups delayed due to storage throttling; database switched to FULL recovery for point-in-time restore, grew rapidly.",
    "category": "Database",
    "status": "In Progress",
    "severity": "High",
    "application": "Azure SQL - FinanceDB",
    "affected_users": "Finance reporting",
    "environment": "Production",
    "solution": "1. Increased log storage and resumed frequent log backups.\n2. Investigated long-running transactions holding VLFs.\n3. Implemented alerting for log utilization.",
    "reasoning": "Backup delays and long transactions prevented log truncation.",
    "timestamp": "2025-11-12 13:50:00"
  },
  {
    "ticket_id": "TKT-0045",
    "title": "DB: PostgreSQL connection refused from Databricks (PG_hba config)",
    "description": "Connections from Databricks VNet blocked; pg_hba.conf not updated after new CIDR allocation.",
    "category": "Database",
    "status": "Resolved",
    "severity": "Medium",
    "application": "PostgreSQL - Analytics",
    "affected_users": "ETL jobs loading to PostgreSQL",
    "environment": "Staging",
    "solution": "1. Allowed new CIDR in pg_hba rules and reloaded config.\n2. Validated SSL and user mapping.\n3. Automated rule updates via IaC.",
    "reasoning": "Network CIDR changed; access rules not updated.",
    "timestamp": "2025-11-12 14:20:00"
  },
  {
    "ticket_id": "TKT-0046",
    "title": "DB: MySQL read replica lag spikes causing stale reads",
    "description": "Replica fell behind during large bulk load; read-after-write consistency broken for user timelines.",
    "category": "Database",
    "status": "Resolved",
    "severity": "Medium",
    "application": "MySQL - SocialFeed",
    "affected_users": "Mobile app users",
    "environment": "Production",
    "solution": "1. Paused non-critical bulk loads during peak.\n2. Moved read traffic to primary for hot keys.\n3. Tuned replica IO thread and increased instance size.",
    "reasoning": "Bulk load overwhelmed replica apply rate.",
    "timestamp": "2025-11-12 14:55:00"
  },
  {
    "ticket_id": "TKT-0047",
    "title": "DB: Oracle dead connection detection misconfigured causing idle sessions",
    "description": "Application pools accumulate stale sessions; server not closing dead connections; pool exhaustion occurs.",
    "category": "Database",
    "status": "In Progress",
    "severity": "Low",
    "application": "Oracle - ERP",
    "affected_users": "Backoffice apps",
    "environment": "Production",
    "solution": "1. Enabled SQLNET.EXPIRE_TIME and validated DCD behavior.\n2. Tuned pool idle timeouts.\n3. Added healthcheck in app to drop dead sessions.",
    "reasoning": "Lack of DCD and idle timeouts led to connection leaks.",
    "timestamp": "2025-11-12 15:30:00"
  },
  {
    "ticket_id": "TKT-0048",
    "title": "DB: Snowflake warehouse suspended mid-query causing 'Session no longer valid'",
    "description": "Auto-suspend kicked in too aggressively; BI dashboards show intermittent failures.",
    "category": "Database",
    "status": "Resolved",
    "severity": "Low",
    "application": "Snowflake - BI Warehouse",
    "affected_users": "BI users",
    "environment": "Production",
    "solution": "1. Increased auto-suspend timeout and enabled auto-resume.\n2. Pooled connections via service to reduce cold starts.\n3. Added query retry on session invalid.",
    "reasoning": "Warehouse suspension during active sessions caused errors.",
    "timestamp": "2025-11-12 16:05:00"
  },
  {
    "ticket_id": "TKT-0049",
    "title": "DB: Redshift VACUUM/ANALYZE missing leading to query slowness",
    "description": "Large tables fragmented; stats stale; long-running queries and queue buildup observed.",
    "category": "Database",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Redshift - Warehouse",
    "affected_users": "Analytics users",
    "environment": "Production",
    "solution": "1. Scheduled VACUUM and ANALYZE jobs.\n2. Optimized sort/dist keys.\n3. Implemented WLM queue adjustments for concurrency.",
    "reasoning": "Missing maintenance caused degraded performance.",
    "timestamp": "2025-11-12 16:40:00"
  },
  {
    "ticket_id": "TKT-0050",
    "title": "DB: Cosmos DB RU exhaustion on partition hot key",
    "description": "Partition keyed by userId experienced hot key; 429 rate limiting impacts write path.",
    "category": "Database",
    "status": "In Progress",
    "severity": "High",
    "application": "Cosmos DB - Events",
    "affected_users": "Event ingest pipeline",
    "environment": "Production",
    "solution": "1. Enabled autoscale and increased RU/s.\n2. Implemented synthetic key to distribute writes.\n3. Backfilled repartition strategy for hot users.",
    "reasoning": "Skewed partitioning created hot partition and RU throttling.",
    "timestamp": "2025-11-12 17:15:00"
  },
  {
    "ticket_id": "TKT-0051",
    "title": "Snowflake external stage to ADLS failing: SAS token invalid",
    "description": "COPY INTO from external stage fails: 'The specified SAS token is invalid'. Secret rotated yesterday; stage still references old token.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "High",
    "application": "Snowflake - External Stage (ADLS)",
    "affected_users": "Ingestion jobs for bronze layer",
    "environment": "Production",
    "solution": "1. Regenerated SAS token with correct permissions (rl).\n2. Updated stage credentials via CREATE OR REPLACE STAGE with new CREDENTIALS.\n3. Added secret rotation runbook and validation procedure.",
    "reasoning": "Stage pointed to expired SAS causing storage auth failure.",
    "timestamp": "2025-11-12 17:45:00"
  },
  {
    "ticket_id": "TKT-0052",
    "title": "Snowflake key pair authentication failing: bad decrypt (PKCS8)",
    "description": "Programmatic login using key pair fails: 'bad decrypt' / 'private key is not in PKCS8 format'. New key deployed with wrong format/passphrase.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - Key Pair Auth",
    "affected_users": "CI pipelines using JDBC",
    "environment": "Staging",
    "solution": "1. Converted private key to PKCS8 and secured passphrase handling.\n2. Updated user public key in Snowflake.\n3. Added CI check to validate key format before deploy.",
    "reasoning": "Mismatched key format/passphrase caused auth failures.",
    "timestamp": "2025-11-12 18:10:00"
  },
  {
    "ticket_id": "TKT-0053",
    "title": "Snowflake OAuth JDBC fails: audience/issuer mismatch",
    "description": "JDBC connection using OAuth returns 'invalid_grant' due to wrong audience (resource) in token; IdP app misconfigured.",
    "category": "Snowflake",
    "status": "In Progress",
    "severity": "High",
    "application": "Snowflake - OAuth",
    "affected_users": "Data apps connecting with SSO",
    "environment": "Production",
    "solution": "1. Corrected IdP app to request Snowflake audience.\n2. Rotated client secret and reconsented scopes.\n3. Added token introspection in pre-connection checks.",
    "reasoning": "Incorrect audience in OAuth token caused Snowflake to reject grant.",
    "timestamp": "2025-11-12 18:35:00"
  },
  {
    "ticket_id": "TKT-0054",
    "title": "Snowflake stored procedure JS error: ReferenceError",
    "description": "JS stored procedure fails: 'ReferenceError: result is not defined' during cleanup routine for late-arriving data.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - Stored Procedures",
    "affected_users": "Data quality remediation jobs",
    "environment": "Staging",
    "solution": "1. Fixed variable scoping and added try/catch with RESULT_SCAN fallback.\n2. Wrote unit tests using Snowpark for procedure logic.\n3. Implemented canary execution before full run.",
    "reasoning": "Undeclared variable in JS procedure caused runtime failure.",
    "timestamp": "2025-11-12 19:00:00"
  },
  {
    "ticket_id": "TKT-0055",
    "title": "Snowflake procedure aborted: warehouse auto-suspended mid-run",
    "description": "Long-running stored procedure canceled with 'Statement canceled because warehouse was suspended'. Auto-suspend set to 5 minutes.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "High",
    "application": "Snowflake - Compute Warehouse",
    "affected_users": "Monthly backfill job",
    "environment": "Production",
    "solution": "1. Increased auto-suspend timeout for warehouse.\n2. Enabled task-managed warehouse for long jobs.\n3. Added keepalive heartbeat inside procedure.",
    "reasoning": "Aggressive auto-suspend interrupted active procedure.",
    "timestamp": "2025-11-12 19:25:00"
  },
  {
    "ticket_id": "TKT-0056",
    "title": "Airflow DAG to Snowflake fails: SQL compilation error (role/object)",
    "description": "SnowflakeOperator fails: 'SQL compilation error: Object does not exist or not authorized: STG.CUSTOMERS'. Airflow connection uses role without USAGE on schema.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "High",
    "application": "Airflow -> Snowflake DAG",
    "affected_users": "Daily ingestion pipeline",
    "environment": "Production",
    "solution": "1. Granted USAGE on database/schema and SELECT/INSERT on tables to Airflow role.\n2. Set ROLE in pre-task SQL.\n3. Added least-privilege role and CI permission checks.",
    "reasoning": "Airflow role lacked necessary privileges on target schema.",
    "timestamp": "2025-11-12 19:50:00"
  },
  {
    "ticket_id": "TKT-0057",
    "title": "Airflow Snowflake load fails due to FILE FORMAT mismatch",
    "description": "COPY INTO errors: 'Numeric value \"true\" is not recognized'. File format defined as CSV; source is JSON lines.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Airflow -> Snowflake DAG",
    "affected_users": "Partner data onboarding",
    "environment": "Staging",
    "solution": "1. Created JSON file format and updated COPY INTO options.\n2. Validated sample files in stage.\n3. Added schema-on-read tests in CI.",
    "reasoning": "Incorrect file format configuration caused parse errors.",
    "timestamp": "2025-11-12 20:15:00"
  },
  {
    "ticket_id": "TKT-0058",
    "title": "Snowpipe stopped ingesting: notification integration disabled",
    "description": "No new files ingested; pipe shows healthy but storage event notifications disabled after storage policy change.",
    "category": "Snowflake",
    "status": "In Progress",
    "severity": "High",
    "application": "Snowflake - Snowpipe",
    "affected_users": "Near-real-time ingestion consumers",
    "environment": "Production",
    "solution": "1. Re-enabled event notifications on storage container.\n2. Validated integration and recreated subscription.\n3. Added monitoring for pipe latency.",
    "reasoning": "Disabled notifications prevented Snowpipe triggers.",
    "timestamp": "2025-11-12 20:40:00"
  },
  {
    "ticket_id": "TKT-0059",
    "title": "Snowflake task chain suspended due to credit quota",
    "description": "Scheduled TASKs show 'suspended by system' after hitting monthly credit quota; downstream refreshes stale.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - Tasks",
    "affected_users": "BI dashboards and incremental loads",
    "environment": "Production",
    "solution": "1. Increased credit quota and added Resource Monitor notifications.\n2. Tuned schedule and consolidated tasks.\n3. Enabled multi-size warehouses for off-peak runs.",
    "reasoning": "Resource monitor suspended tasks on quota breach.",
    "timestamp": "2025-11-12 21:05:00"
  },
  {
    "ticket_id": "TKT-0060",
    "title": "Snowflake stream lag causing delayed MERGE",
    "description": "Stream shows high offset lag; downstream MERGE job reads same offsets repeatedly due to low DML and retention nearing limit.",
    "category": "Snowflake",
    "status": "In Progress",
    "severity": "Low",
    "application": "Snowflake - Streams",
    "affected_users": "Downstream silver tables",
    "environment": "Staging",
    "solution": "1. Increased stream retention and added watermark checks.\n2. Adjusted job cadence to align with upstream DML.\n3. Added alerting for stream staleness.",
    "reasoning": "Low upstream DML and retention window mismatch delayed consumption.",
    "timestamp": "2025-11-12 21:30:00"
  },
  {
    "ticket_id": "TKT-0061",
    "title": "Snowflake insufficient privileges to CREATE in schema",
    "description": "CTAS step fails with 'Insufficient privileges to operate on schema'; role missing CREATE TABLE on target schema.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "High",
    "application": "Snowflake - DDL",
    "affected_users": "ETL deployment pipeline",
    "environment": "Production",
    "solution": "1. Granted CREATE TABLE and OWNERSHIP to deployment role where appropriate.\n2. Implemented role hierarchy policy.\n3. Added permissions smoke test pre-deploy.",
    "reasoning": "Role lacked DDL privileges required by pipeline.",
    "timestamp": "2025-11-12 21:55:00"
  },
  {
    "ticket_id": "TKT-0062",
    "title": "Snowflake warehouse queuing spikes during morning loads",
    "description": "Queued queries observed; warehouse scaling policy set to 'economy' with max cluster size 1; SLA breaches at 8-9am.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - Warehouses",
    "affected_users": "Morning batch consumers",
    "environment": "Production",
    "solution": "1. Switched scaling policy to 'standard' and increased max clusters.\n2. Implemented query concurrency controls.\n3. Split workload into dedicated warehouses.",
    "reasoning": "Under-provisioned warehouse caused long queues under predictable spikes.",
    "timestamp": "2025-11-12 22:20:00"
  },
  {
    "ticket_id": "TKT-0063",
    "title": "Snowflake ADLS stage path incorrect (dfs vs blob endpoint)",
    "description": "Stage configured with dfs.core.windows.net but storage account uses blob endpoint for compatibility; COPY INTO errors 'resource does not exist'.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Low",
    "application": "Snowflake - External Stage",
    "affected_users": "Ad-hoc loads",
    "environment": "Development",
    "solution": "1. Corrected endpoint to match storage configuration.\n2. Added stage validation script to test list/status.\n3. Documented endpoint selection rules.",
    "reasoning": "Endpoint mismatch led to path resolution failures.",
    "timestamp": "2025-11-12 22:45:00"
  },
  {
    "ticket_id": "TKT-0064",
    "title": "Snowflake S3 stage region mismatch",
    "description": "Access denied with message instructing to use correct region; bucket in eu-west-1 but client hits us-east-1 endpoints.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - External Stage (S3)",
    "affected_users": "Global ingestion jobs",
    "environment": "Staging",
    "solution": "1. Set REGION parameter on stage to eu-west-1.\n2. Validated endpoint routing and credentials.\n3. Added IaC guardrail for region alignment.",
    "reasoning": "Stage attempted to access bucket via wrong region endpoints.",
    "timestamp": "2025-11-12 23:10:00"
  },
  {
    "ticket_id": "TKT-0065",
    "title": "Snowflake decrypt error on client-side encrypted files",
    "description": "COPY INTO fails with 'DECRYPTION_FAILED_OR_BAD_RECORD_MAC' for KMS-encrypted files; wrong encryption key specified in stage.",
    "category": "Snowflake",
    "status": "In Progress",
    "severity": "High",
    "application": "Snowflake - Encrypted Loads",
    "affected_users": "Security-sensitive data ingestion",
    "environment": "Production",
    "solution": "1. Verified KMS key IDs and rotated data key mapping.\n2. Re-encrypted sample files with correct key to validate.\n3. Implemented key-id checks in pipeline.",
    "reasoning": "Mismatched encryption keys prevented decryption on load.",
    "timestamp": "2025-11-12 23:35:00"
  },
  {
    "ticket_id": "TKT-0066",
    "title": "Snowflake UNLOAD to S3 failing: AccessDenied (s3:PutObjectAcl)",
    "description": "UNLOAD statements fail with AccessDenied; IAM role lacks PutObjectAcl permission required by bucket policy.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - External Unload",
    "affected_users": "Export jobs for data sharing",
    "environment": "Production",
    "solution": "1. Granted s3:PutObjectAcl to stage IAM role per bucket policy.\n2. Validated UNLOAD with small sample.\n3. Added IAM policy tests in CI.",
    "reasoning": "Bucket policy required ACL write; role lacked permission.",
    "timestamp": "2025-11-13 00:00:00"
  },
  {
    "ticket_id": "TKT-0067",
    "title": "Snowflake materialized view refresh lag and credit spike",
    "description": "MV refreshes every minute on large base table; credit usage spiked and staleness persists under heavy DML.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Low",
    "application": "Snowflake - Materialized Views",
    "affected_users": "BI dashboards",
    "environment": "Production",
    "solution": "1. Reduced refresh frequency and optimized base table clustering.\n2. Converted MV to task-driven incremental table.\n3. Implemented cost guardrails for MV size.",
    "reasoning": "Over-frequent refresh on volatile table caused high cost with limited freshness gain.",
    "timestamp": "2025-11-13 00:25:00"
  },
  {
    "ticket_id": "TKT-0068",
    "title": "Airflow DAG canceled queries due to Resource Monitor",
    "description": "Airflow job frequently sees 'Statement canceled due to warehouse credit quota exceeded'; runs fail mid-load near month-end.",
    "category": "Snowflake",
    "status": "In Progress",
    "severity": "High",
    "application": "Airflow -> Snowflake",
    "affected_users": "All downstream consumers relying on daily loads",
    "environment": "Production",
    "solution": "1. Adjusted Resource Monitor thresholds and notifications.\n2. Shifted heavy jobs to lower-cost hours and separate warehouses.\n3. Added retries with resume from checkpoint.",
    "reasoning": "Credit cap hit during peak jobs canceling active queries.",
    "timestamp": "2025-11-13 00:50:00"
  },
  {
    "ticket_id": "TKT-0069",
    "title": "Snowflake MERGE produced duplicates due to weak match predicate",
    "description": "MERGE INTO silver table generated duplicate rows; match used non-unique business key without tenant scoping.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "High",
    "application": "Snowflake - DML",
    "affected_users": "Data consumers of silver layer",
    "environment": "Staging",
    "solution": "1. Fixed MERGE predicate to include tenantId and natural key uniqueness.\n2. Added NOT NULL/UNIQUE constraints where applicable.\n3. Backfilled de-dup and audit checks.",
    "reasoning": "Non-deterministic match condition allowed multiple matches and duplicates.",
    "timestamp": "2025-11-13 01:15:00"
  },
  {
    "ticket_id": "TKT-0070",
    "title": "Snowflake VARIANT parse error on malformed JSON",
    "description": "COPY INTO fails 'Invalid value for cast to VARIANT: unexpected token' due to vendor files with stray BOM and truncated lines.",
    "category": "Snowflake",
    "status": "Resolved",
    "severity": "Medium",
    "application": "Snowflake - Semi-structured Data",
    "affected_users": "Vendor data ingestion",
    "environment": "Production",
    "solution": "1. Enabled STRIP_OUTER_ARRAY and set UTF8 encoding; removed BOM.\n2. Validated JSON with pre-ingest sanitizer.\n3. Added reject limit and quarantine stage.",
    "reasoning": "Malformed JSON and BOM characters caused VARIANT parsing to fail.",
    "timestamp": "2025-11-13 01:40:00"
  }
]
