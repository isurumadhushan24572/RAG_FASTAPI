{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304d845f",
   "metadata": {},
   "source": [
    "# RAG Application Evaluation Using RAGAS\n",
    "\n",
    "This notebook evaluates the performance and accuracy of the RAG-based support ticket system using the RAGAS framework.\n",
    "\n",
    "## Metrics Evaluated:\n",
    "- **Faithfulness**: How grounded the answer is in the retrieved context\n",
    "- **Answer Relevancy**: How relevant the answer is to the question\n",
    "- **Context Precision**: Quality of retrieved contexts\n",
    "- **Context Recall**: Coverage of ground truth in retrieved contexts\n",
    "- **Answer Correctness**: Semantic and factual correctness\n",
    "- **Answer Similarity**: Semantic similarity to ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04395168",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this cell first if packages are not already installed\n",
    "# !pip install ragas datasets langchain langchain-groq langchain-google-genai sentence-transformers pandas matplotlib seaborn requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ceba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RAGAS imports\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "# LangChain imports for LLM setup (Groq and Gemini)\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae3555d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG API URL: http://localhost:8000\n",
      "LLM Provider: groq\n",
      "LLM Model: llama-3.3-70b-versatile\n",
      "LLM Provider: gemini\n",
      "LLM Model: gemini-2.5-flash\n",
      "Tickets file: ./rag-frontend/Json_Files/tickets_sample.json\n",
      "Sample queries file: ./rag-frontend/Json_Files/sample.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('./rag-backend/.env')\n",
    "\n",
    "# API Configuration\n",
    "RAG_API_BASE_URL = \"http://localhost:8000\"  # Your FastAPI backend\n",
    "\n",
    "# LLM API Keys - RAGAS will use these for evaluation\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', '')  # For Gemini\n",
    "\n",
    "# File paths\n",
    "TICKETS_FILE = './rag-frontend/Json_Files/tickets_sample.json'\n",
    "SAMPLE_QUERIES_FILE = './rag-frontend/Json_Files/sample.json'\n",
    "\n",
    "# LLM Choice for RAGAS evaluation\n",
    "LLM_PROVIDER1 = \"groq\"  # Options: \"groq\" or \"gemini\"\n",
    "LLM_MODEL1 = \"llama-3.3-70b-versatile\"  # For Groq: llama-3.1-70b-versatile, mixtral-8x7b-32768\n",
    "# For Gemini: gemini-1.5-pro, gemini-1.5-flash\n",
    "LLM_PROVIDER2 = \"gemini\"\n",
    "LLM_MODEL2 = \"gemini-2.5-flash\"\n",
    "\n",
    "print(f\"RAG API URL: {RAG_API_BASE_URL}\")\n",
    "print(f\"LLM Provider: {LLM_PROVIDER1}\")\n",
    "print(f\"LLM Model: {LLM_MODEL1}\")\n",
    "print(f\"LLM Provider: {LLM_PROVIDER2}\")\n",
    "print(f\"LLM Model: {LLM_MODEL2}\")\n",
    "print(f\"Tickets file: {TICKETS_FILE}\")\n",
    "print(f\"Sample queries file: {SAMPLE_QUERIES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ae291",
   "metadata": {},
   "source": [
    "### LLM Configuration Guide\n",
    "\n",
    "**Available Options:**\n",
    "\n",
    "1. **Groq (Recommended - Fast & Free)**\n",
    "   - Set `LLM_PROVIDER = \"groq\"`\n",
    "   - Add `GROQ_API_KEY` to `.env` file\n",
    "   - Models: `llama-3.1-70b-versatile`, `llama-3.1-8b-instant`, `mixtral-8x7b-32768`\n",
    "   - Get API key: https://console.groq.com/\n",
    "\n",
    "2. **Google Gemini (Alternative)**\n",
    "   - Set `LLM_PROVIDER = \"gemini\"`\n",
    "   - Add `GOOGLE_API_KEY` to `.env` file\n",
    "   - Models: `gemini-1.5-pro`, `gemini-1.5-flash`\n",
    "   - Get API key: https://ai.google.dev/\n",
    "\n",
    "**Note:** RAGAS requires an LLM to evaluate responses. Choose one of the above providers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e823e9a",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ebb38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 50 tickets from vector DB data\n",
      "\n",
      "First ticket example:\n",
      "ID: TKT-0001\n",
      "Title: Payment API Returning 500 Internal Server Error\n",
      "Status: Resolved\n"
     ]
    }
   ],
   "source": [
    "# Load tickets (ground truth data from vector DB)\n",
    "with open(TICKETS_FILE, 'r', encoding='utf-8') as f:\n",
    "    tickets_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(tickets_data)} tickets from vector DB data\")\n",
    "print(f\"\\nFirst ticket example:\")\n",
    "print(f\"ID: {tickets_data[0]['ticket_id']}\")\n",
    "print(f\"Title: {tickets_data[0]['title']}\")\n",
    "print(f\"Status: {tickets_data[0]['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089a91ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 20 sample queries\n",
      "\n",
      "First query example:\n",
      "Title: Payment API Throwing 500 Errors\n",
      "Description: Payment processing API returning 500 internal server errors for all payment transactions. Started ab...\n"
     ]
    }
   ],
   "source": [
    "# Load sample user queries\n",
    "with open(SAMPLE_QUERIES_FILE, 'r', encoding='utf-8') as f:\n",
    "    sample_queries = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(sample_queries)} sample queries\")\n",
    "print(f\"\\nFirst query example:\")\n",
    "print(f\"Title: {sample_queries[0]['title']}\")\n",
    "print(f\"Description: {sample_queries[0]['description'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637f1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tickets Dataset Statistics ===\n",
      "Total tickets: 50\n",
      "\n",
      "Categories:\n",
      "category\n",
      "Monitoring        32\n",
      "Infrastructure     6\n",
      "API Issues         3\n",
      "Performance        3\n",
      "Security           2\n",
      "Database           1\n",
      "DevOps             1\n",
      "Networking         1\n",
      "Storage            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Severity:\n",
      "severity\n",
      "High        23\n",
      "Medium      16\n",
      "Critical     8\n",
      "Low          3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Status:\n",
      "status\n",
      "Resolved       30\n",
      "In Progress    15\n",
      "Open            5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display data statistics\n",
    "tickets_df = pd.DataFrame(tickets_data)\n",
    "print(\"\\n=== Tickets Dataset Statistics ===\")\n",
    "print(f\"Total tickets: {len(tickets_df)}\")\n",
    "print(f\"\\nCategories:\")\n",
    "print(tickets_df['category'].value_counts())\n",
    "print(f\"\\nSeverity:\")\n",
    "print(tickets_df['severity'].value_counts())\n",
    "print(f\"\\nStatus:\")\n",
    "print(tickets_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385f597",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Dataset\n",
    "\n",
    "Map sample queries to their corresponding ground truth solutions from the tickets database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a6cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created evaluation dataset with 20 test cases\n",
      "\n",
      "Example test case:\n",
      "Question: Payment API Throwing 500 Errors. Payment processing API returning 500 internal server errors for all...\n",
      "Expected Ticket: TKT-0001\n"
     ]
    }
   ],
   "source": [
    "def create_evaluation_dataset(sample_queries: List[Dict], tickets_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create evaluation dataset by mapping sample queries to ground truth from tickets.\n",
    "    \n",
    "    Each entry contains:\n",
    "    - question: User query\n",
    "    - ground_truth: Expected solution from ticket\n",
    "    - expected_ticket_id: Ticket that should be retrieved\n",
    "    \"\"\"\n",
    "    eval_dataset = []\n",
    "    \n",
    "    # Create mapping based on similar content\n",
    "    # Assuming samples correspond to first N tickets\n",
    "    for idx, sample in enumerate(sample_queries):\n",
    "        if idx < len(tickets_data):\n",
    "            ticket = tickets_data[idx]\n",
    "            \n",
    "            # Construct question from sample\n",
    "            question = f\"{sample['title']}. {sample['description']}\"\n",
    "            \n",
    "            # Construct ground truth from ticket\n",
    "            ground_truth = (\n",
    "                f\"Solution: {ticket['solution']}\\n\\n\"\n",
    "                f\"Reasoning: {ticket['reasoning']}\\n\\n\"\n",
    "                f\"Category: {ticket['category']}\\n\"\n",
    "                f\"Severity: {ticket['severity']}\"\n",
    "            )\n",
    "            \n",
    "            eval_dataset.append({\n",
    "                'question': question,\n",
    "                'ground_truth': ground_truth,\n",
    "                'expected_ticket_id': ticket['ticket_id'],\n",
    "                'expected_title': ticket['title'],\n",
    "                'category': ticket['category'],\n",
    "                'severity': ticket['severity']\n",
    "            })\n",
    "    \n",
    "    return eval_dataset\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = create_evaluation_dataset(sample_queries, tickets_data)\n",
    "print(f\"‚úÖ Created evaluation dataset with {len(eval_dataset)} test cases\")\n",
    "print(f\"\\nExample test case:\")\n",
    "print(f\"Question: {eval_dataset[0]['question'][:100]}...\")\n",
    "print(f\"Expected Ticket: {eval_dataset[0]['expected_ticket_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472229f",
   "metadata": {},
   "source": [
    "## 4. Query RAG System\n",
    "\n",
    "Query the RAG API to get responses and retrieved contexts for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca7658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG API connection...\n",
      "Endpoint: http://localhost:8000/api/v1/tickets/search\n",
      "\n",
      "API Status: ‚úÖ Connected\n",
      "Contexts retrieved: 3\n",
      "Retrieved ticket IDs: ['TKT-0001', 'TKT-0009', 'TKT-0017']\n"
     ]
    }
   ],
   "source": [
    "def query_rag_system(question: str, api_url: str = RAG_API_BASE_URL) -> Dict:\n",
    "    \"\"\"\n",
    "    Query the RAG system API and return the response with contexts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'answer', 'contexts', and 'metadata'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use GET request with query parameters (matches your backend API)\n",
    "        response = requests.get(\n",
    "            f\"{api_url}/api/v1/tickets/search\",\n",
    "            params={\n",
    "                \"query\": question,\n",
    "                \"limit\": 3,\n",
    "                \"collection_name\": \"SupportTickets\"\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract contexts from retrieved documents\n",
    "            contexts = []\n",
    "            retrieved_ids = []\n",
    "            \n",
    "            # Your backend returns 'results' array with ticket data\n",
    "            if 'results' in data:\n",
    "                for doc in data['results']:\n",
    "                    # Combine relevant fields for context\n",
    "                    context = (\n",
    "                        f\"Ticket ID: {doc.get('ticket_id', 'N/A')}\\n\"\n",
    "                        f\"Title: {doc.get('title', '')}\\n\"\n",
    "                        f\"Description: {doc.get('description', '')}\\n\"\n",
    "                        f\"Solution: {doc.get('solution', '')}\\n\"\n",
    "                        f\"Reasoning: {doc.get('reasoning', '')}\"\n",
    "                    )\n",
    "                    contexts.append(context)\n",
    "                    retrieved_ids.append(doc.get('ticket_id', 'N/A'))\n",
    "            \n",
    "            return {\n",
    "                'answer': data.get('answer', ''),  # Your API may not return 'answer' for search\n",
    "                'contexts': contexts if contexts else ['No context retrieved'],\n",
    "                'retrieved_ticket_ids': retrieved_ids,\n",
    "                'success': True\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  API returned status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text[:200]}\")\n",
    "            return {\n",
    "                'answer': 'Error: API request failed',\n",
    "                'contexts': ['Error retrieving contexts'],\n",
    "                'retrieved_ticket_ids': [],\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying RAG system: {str(e)}\")\n",
    "        return {\n",
    "            'answer': f'Error: {str(e)}',\n",
    "            'contexts': ['Error retrieving contexts'],\n",
    "            'retrieved_ticket_ids': [],\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "# Test the connection\n",
    "print(\"Testing RAG API connection...\")\n",
    "print(f\"Endpoint: {RAG_API_BASE_URL}/api/v1/tickets/search\")\n",
    "test_response = query_rag_system(eval_dataset[0]['question'])\n",
    "print(f\"\\nAPI Status: {'‚úÖ Connected' if test_response['success'] else '‚ùå Failed'}\")\n",
    "if test_response['success']:\n",
    "    print(f\"Contexts retrieved: {len(test_response['contexts'])}\")\n",
    "    print(f\"Retrieved ticket IDs: {test_response['retrieved_ticket_ids']}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Troubleshooting:\")\n",
    "    print(\"  1. Ensure backend is running: docker-compose up\")\n",
    "    print(\"  2. Check health endpoint: http://localhost:8000/health\")\n",
    "    print(\"  3. Verify collection exists in Weaviate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a5b6ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying RAG system for all test cases...\n",
      "\n",
      "Processing 1/20: TKT-0001\n",
      "Processing 2/20: TKT-0002\n",
      "Processing 3/20: TKT-0003\n",
      "Processing 4/20: TKT-0004\n",
      "Processing 5/20: TKT-0005\n",
      "Processing 6/20: TKT-0006\n",
      "Processing 7/20: TKT-0007\n",
      "Processing 8/20: TKT-0008\n",
      "Processing 9/20: TKT-0009\n",
      "Processing 10/20: TKT-0010\n",
      "Processing 11/20: TKT-0011\n",
      "Processing 12/20: TKT-0012\n",
      "Processing 13/20: TKT-0013\n",
      "Processing 14/20: TKT-0014\n",
      "Processing 15/20: TKT-0015\n",
      "Processing 16/20: TKT-0016\n",
      "Processing 17/20: TKT-0017\n",
      "Processing 18/20: TKT-0018\n",
      "Processing 19/20: TKT-0019\n",
      "Processing 20/20: TKT-0020\n",
      "\n",
      "‚úÖ Collected 20 RAG responses\n",
      "Successful queries: 20\n"
     ]
    }
   ],
   "source": [
    "# Query RAG system for all test cases\n",
    "print(\"Querying RAG system for all test cases...\\n\")\n",
    "\n",
    "rag_responses = []\n",
    "for idx, test_case in enumerate(eval_dataset):\n",
    "    print(f\"Processing {idx + 1}/{len(eval_dataset)}: {test_case['expected_ticket_id']}\")\n",
    "    \n",
    "    response = query_rag_system(test_case['question'])\n",
    "    \n",
    "    rag_responses.append({\n",
    "        'question': test_case['question'],\n",
    "        'answer': response['answer'],\n",
    "        'contexts': response['contexts'],\n",
    "        'ground_truth': test_case['ground_truth'],\n",
    "        'expected_ticket_id': test_case['expected_ticket_id'],\n",
    "        'retrieved_ticket_ids': response['retrieved_ticket_ids'],\n",
    "        'success': response['success']\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Collected {len(rag_responses)} RAG responses\")\n",
    "print(f\"Successful queries: {sum(1 for r in rag_responses if r['success'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56898bb2",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset for RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27fef95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created RAGAS dataset with 20 samples\n",
      "\n",
      "Dataset structure:\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert to RAGAS format\n",
    "ragas_data = {\n",
    "    'question': [r['question'] for r in rag_responses],\n",
    "    'answer': [r['answer'] for r in rag_responses],\n",
    "    'contexts': [r['contexts'] for r in rag_responses],\n",
    "    'ground_truth': [r['ground_truth'] for r in rag_responses]\n",
    "}\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "ragas_dataset = Dataset.from_dict(ragas_data)\n",
    "\n",
    "print(f\"‚úÖ Created RAGAS dataset with {len(ragas_dataset)} samples\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(ragas_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d91f8",
   "metadata": {},
   "source": [
    "## 6. Run RAGAS Evaluation\n",
    "\n",
    "Evaluate the RAG system using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb571370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURING LLM FOR RAGAS EVALUATION\n",
      "============================================================\n",
      "‚úÖ Using Groq LLM: llama-3.3-70b-versatile\n",
      "‚úÖ Groq LLM initialized successfully!\n",
      "\n",
      "üìä Initializing embeddings model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ISURU\\AppData\\Local\\Temp\\ipykernel_15824\\2907632572.py:43: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings initialized!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM for RAGAS evaluation (Groq or Gemini)\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURING LLM FOR RAGAS EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "llm = None\n",
    "\n",
    "if LLM_PROVIDER1.lower() == \"groq\" and GROQ_API_KEY:  # changed\n",
    "    print(f\"‚úÖ Using Groq LLM: {LLM_MODEL1}\")\n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            model=LLM_MODEL1,\n",
    "            api_key=GROQ_API_KEY,\n",
    "            temperature=0,\n",
    "            max_retries=3,\n",
    "        )\n",
    "        print(\"‚úÖ Groq LLM initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Groq: {e}\")\n",
    "\n",
    "elif LLM_PROVIDER2.lower() == \"gemini\" and GOOGLE_API_KEY:\n",
    "    print(f\"‚úÖ Using Google Gemini: {LLM_MODEL2}\")\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=LLM_MODEL2 if LLM_MODEL2.startswith(\"gemini\") else \"gemini-1.5-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            temperature=0,\n",
    "            max_retries=3,\n",
    "        )\n",
    "        print(\"‚úÖ Gemini LLM initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Gemini: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No valid API key found!\")\n",
    "    print(\"Please set one of the following in your .env file:\")\n",
    "    print(\"  - GROQ_API_KEY (for Groq/Llama models)\")\n",
    "    print(\"  - GOOGLE_API_KEY (for Gemini models)\")\n",
    "    print(\"\\nRAGAS requires an LLM for evaluation.\")\n",
    "\n",
    "# Initialize embeddings for RAGAS\n",
    "print(\"\\nüìä Initializing embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "print(\"‚úÖ Embeddings initialized!\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d481a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing LLM connection...\n",
      "‚úÖ LLM Response: Hello, LLM is working.\n",
      "‚úÖ LLM is ready for RAGAS evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Test LLM connection\n",
    "if llm:\n",
    "    print(\"\\nüß™ Testing LLM connection...\")\n",
    "    try:\n",
    "        test_response = llm.invoke(\"Say 'Hello! LLM is working.' in one sentence.\")\n",
    "        print(f\"‚úÖ LLM Response: {test_response.content}\")\n",
    "        print(f\"‚úÖ LLM is ready for RAGAS evaluation!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM test failed: {e}\")\n",
    "        print(\"Please check your API key and internet connection.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping LLM test - No LLM configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddaecf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics to evaluate:\n",
      "  - faithfulness\n",
      "  - answer_relevancy\n",
      "  - context_precision\n",
      "  - context_recall\n",
      "  - answer_correctness\n",
      "  - answer_similarity\n"
     ]
    }
   ],
   "source": [
    "# Define metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,           # Is answer grounded in retrieved context?\n",
    "    answer_relevancy,       # Does answer address the question?\n",
    "    context_precision,      # Are relevant contexts ranked higher?\n",
    "    context_recall,         # Do contexts cover the ground truth?\n",
    "    answer_correctness,     # Semantic + factual correctness\n",
    "    answer_similarity       # Semantic similarity to ground truth\n",
    "]\n",
    "\n",
    "print(\"Metrics to evaluate:\")\n",
    "for metric in metrics:\n",
    "    print(f\"  - {metric.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7489d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running RAGAS evaluation...\n",
      "\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ad509e14c4caf9d98c3214d8618dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"\\nüöÄ Running RAGAS evaluation...\\n\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "if llm is None:\n",
    "    print(\"‚ùå Cannot run evaluation: No LLM configured!\")\n",
    "    print(\"Please configure GROQ_API_KEY or GOOGLE_API_KEY in your .env file\")\n",
    "    results = None\n",
    "else:\n",
    "    try:\n",
    "        # Run evaluation with configured LLM\n",
    "        results = evaluate(\n",
    "            ragas_dataset,\n",
    "            metrics=metrics,\n",
    "            llm=llm,\n",
    "            embeddings=embeddings,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Evaluation failed: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Ensure you have a valid API key (Groq or Google)\")\n",
    "        print(\"2. Check your internet connection\")\n",
    "        print(\"3. Verify the RAG API is running and accessible\")\n",
    "        print(\"4. Make sure the selected model is available:\")\n",
    "        print(f\"   - Current LLM: {LLM_PROVIDER1} - {LLM_MODEL1}\")\n",
    "        # print(\"\\nFor Groq, available models:\")\n",
    "        # print(\"  - llama-3.1-70b-versatile\")\n",
    "        # print(\"  - llama-3.1-8b-instant\")\n",
    "        # print(\"  - mixtral-8x7b-32768\")\n",
    "        # print(\"\\nFor Gemini, available models:\")\n",
    "        # print(\"  - gemini-1.5-pro\")\n",
    "        # print(\"  - gemini-1.5-flash\")\n",
    "        results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358fcaf",
   "metadata": {},
   "source": [
    "## 7. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Display overall scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG SYSTEM EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìä Overall Metrics (0.0 - 1.0 scale):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for metric_name, score in results.items():\n",
    "        if metric_name != 'question':  # Skip the question field\n",
    "            # Determine status emoji\n",
    "            if score >= 0.8:\n",
    "                status = \"üü¢ Excellent\"\n",
    "            elif score >= 0.6:\n",
    "                status = \"üü° Good\"\n",
    "            elif score >= 0.4:\n",
    "                status = \"üü† Fair\"\n",
    "            else:\n",
    "                status = \"üî¥ Needs Improvement\"\n",
    "            \n",
    "            print(f\"{metric_name:25s}: {score:.4f}  {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Convert to DataFrame for detailed analysis\n",
    "    results_df = results.to_pandas()\n",
    "    \n",
    "    # Add metadata\n",
    "    results_df['expected_ticket_id'] = [r['expected_ticket_id'] for r in rag_responses]\n",
    "    results_df['retrieved_ticket_ids'] = [r['retrieved_ticket_ids'] for r in rag_responses]\n",
    "    results_df['success'] = [r['success'] for r in rag_responses]\n",
    "    \n",
    "    # Display detailed results\n",
    "    print(\"\\nüìã Detailed Results Per Test Case:\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_file = f'ragas_evaluation_results_{timestamp}.csv'\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d16c7",
   "metadata": {},
   "source": [
    "## 8. Custom Metrics Analysis\n",
    "\n",
    "Additional metrics specific to support ticket retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate retrieval accuracy\n",
    "def calculate_retrieval_accuracy(responses: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate if the correct ticket was retrieved (Precision@K)\n",
    "    \"\"\"\n",
    "    total = len(responses)\n",
    "    top1_correct = 0  # Exact match at rank 1\n",
    "    top3_correct = 0  # Exact match in top 3\n",
    "    \n",
    "    for response in responses:\n",
    "        expected = response['expected_ticket_id']\n",
    "        retrieved = response['retrieved_ticket_ids']\n",
    "        \n",
    "        if retrieved:\n",
    "            # Check if expected ticket is in top 1\n",
    "            if retrieved[0] == expected:\n",
    "                top1_correct += 1\n",
    "                top3_correct += 1\n",
    "            # Check if expected ticket is in top 3\n",
    "            elif expected in retrieved[:3]:\n",
    "                top3_correct += 1\n",
    "    \n",
    "    return {\n",
    "        'precision_at_1': top1_correct / total if total > 0 else 0,\n",
    "        'precision_at_3': top3_correct / total if total > 0 else 0,\n",
    "        'total_queries': total\n",
    "    }\n",
    "\n",
    "# Calculate custom metrics\n",
    "retrieval_metrics = calculate_retrieval_accuracy(rag_responses)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUSTOM RETRIEVAL METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrecision@1 (Top-1 Accuracy): {retrieval_metrics['precision_at_1']:.2%}\")\n",
    "print(f\"Precision@3 (Top-3 Accuracy): {retrieval_metrics['precision_at_3']:.2%}\")\n",
    "print(f\"Total Queries Evaluated: {retrieval_metrics['total_queries']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430984ee",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c861ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Set style\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Overall Metrics Bar Chart\n",
    "    ax1 = axes[0, 0]\n",
    "    metric_names = [k for k in results.keys() if k != 'question']\n",
    "    metric_values = [results[k] for k in metric_names]\n",
    "    \n",
    "    bars = ax1.barh(metric_names, metric_values, color='skyblue')\n",
    "    ax1.set_xlabel('Score', fontweight='bold')\n",
    "    ax1.set_title('Overall RAGAS Metrics', fontweight='bold')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, metric_values)):\n",
    "        ax1.text(val + 0.02, i, f'{val:.3f}', va='center')\n",
    "    \n",
    "    # Add threshold line\n",
    "    ax1.axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='Good (0.7)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Metric Distribution Box Plot\n",
    "    ax2 = axes[0, 1]\n",
    "    metric_data = [results_df[col].dropna() for col in results_df.columns if col not in ['question', 'expected_ticket_id', 'retrieved_ticket_ids', 'success']]\n",
    "    metric_labels = [col.replace('_', ' ').title() for col in results_df.columns if col not in ['question', 'expected_ticket_id', 'retrieved_ticket_ids', 'success']]\n",
    "    \n",
    "    bp = ax2.boxplot(metric_data, labels=metric_labels, patch_artist=True)\n",
    "    ax2.set_ylabel('Score', fontweight='bold')\n",
    "    ax2.set_title('Score Distribution by Metric', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Retrieval Accuracy\n",
    "    ax3 = axes[1, 0]\n",
    "    retrieval_data = [\n",
    "        retrieval_metrics['precision_at_1'],\n",
    "        retrieval_metrics['precision_at_3']\n",
    "    ]\n",
    "    retrieval_labels = ['Precision@1', 'Precision@3']\n",
    "    colors = ['#ff9999', '#66b3ff']\n",
    "    \n",
    "    bars = ax3.bar(retrieval_labels, retrieval_data, color=colors)\n",
    "    ax3.set_ylabel('Accuracy', fontweight='bold')\n",
    "    ax3.set_title('Retrieval Accuracy', fontweight='bold')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, val in zip(bars, retrieval_data):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{val:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Performance Summary (Radar Chart)\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.remove()  # Remove the axis\n",
    "    ax4 = fig.add_subplot(2, 2, 4, projection='polar')\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    categories = metric_names\n",
    "    values = metric_values\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax4.plot(angles, values, 'o-', linewidth=2, color='b', label='RAG System')\n",
    "    ax4.fill(angles, values, alpha=0.25, color='b')\n",
    "    ax4.set_xticks(angles[:-1])\n",
    "    ax4.set_xticklabels([c.replace('_', '\\n') for c in categories], size=8)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_title('Performance Profile', fontweight='bold', pad=20)\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    fig_file = f'ragas_evaluation_visualization_{timestamp}.png'\n",
    "    plt.savefig(fig_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nüìä Visualization saved to: {fig_file}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f343be5",
   "metadata": {},
   "source": [
    "## 10. Detailed Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Add category information to results\n",
    "    results_df['category'] = [eval_dataset[i]['category'] for i in range(len(eval_dataset))]\n",
    "    results_df['severity'] = [eval_dataset[i]['severity'] for i in range(len(eval_dataset))]\n",
    "    \n",
    "    # Group by category\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE BY CATEGORY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    category_metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "    \n",
    "    for category in results_df['category'].unique():\n",
    "        category_data = results_df[results_df['category'] == category]\n",
    "        print(f\"\\nüìÅ {category}:\")\n",
    "        print(f\"   Number of cases: {len(category_data)}\")\n",
    "        \n",
    "        for metric in category_metrics:\n",
    "            if metric in category_data.columns:\n",
    "                avg_score = category_data[metric].mean()\n",
    "                print(f\"   {metric:20s}: {avg_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1649c78",
   "metadata": {},
   "source": [
    "## 11. Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Identify cases with low scores\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FAILURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        if metric in results_df.columns:\n",
    "            low_scores = results_df[results_df[metric] < threshold]\n",
    "            \n",
    "            if len(low_scores) > 0:\n",
    "                print(f\"\\n‚ö†Ô∏è  Cases with low {metric} (< {threshold}):\")\n",
    "                print(f\"   Count: {len(low_scores)} / {len(results_df)}\")\n",
    "                \n",
    "                for idx, row in low_scores.iterrows():\n",
    "                    print(f\"   - Case {idx}: {row['expected_ticket_id']} (Score: {row[metric]:.3f})\")\n",
    "                    print(f\"     Category: {row['category']}, Severity: {row['severity']}\")\n",
    "            else:\n",
    "                print(f\"\\n‚úÖ All cases passed {metric} threshold ({threshold})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa4344",
   "metadata": {},
   "source": [
    "## 12. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Generate comprehensive summary\n",
    "    summary = {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'total_test_cases': len(rag_responses),\n",
    "        'successful_queries': sum(1 for r in rag_responses if r['success']),\n",
    "        'ragas_metrics': {k: float(v) for k, v in results.items() if k != 'question'},\n",
    "        'retrieval_metrics': retrieval_metrics,\n",
    "        'category_breakdown': results_df.groupby('category').size().to_dict(),\n",
    "        'severity_breakdown': results_df.groupby('severity').size().to_dict(),\n",
    "    }\n",
    "    \n",
    "    # Save summary as JSON\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = f'evaluation_summary_{timestamp}.json'\n",
    "    \n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Summary report saved to: {summary_file}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14da089",
   "metadata": {},
   "source": [
    "## 13. Recommendations\n",
    "\n",
    "Based on the evaluation results, generate actionable recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Check each metric and provide recommendations\n",
    "    if results.get('faithfulness', 0) < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìå Low Faithfulness: The model may be hallucinating. Consider:\\n\"\n",
    "            \"   - Improving prompt engineering to stay grounded in context\\n\"\n",
    "            \"   - Adding source citation requirements\\n\"\n",
    "            \"   - Using more conservative generation parameters\"\n",
    "        )\n",
    "    \n",
    "    if results.get('context_precision', 0) < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìå Low Context Precision: Retrieved contexts may not be relevant. Consider:\\n\"\n",
    "            \"   - Improving embedding model quality\\n\"\n",
    "            \"   - Refining chunk size and overlap\\n\"\n",
    "            \"   - Adding metadata filtering\"\n",
    "        )\n",
    "    \n",
    "    if results.get('context_recall', 0) < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìå Low Context Recall: Missing relevant information. Consider:\\n\"\n",
    "            \"   - Increasing number of retrieved documents (top_k)\\n\"\n",
    "            \"   - Improving document chunking strategy\\n\"\n",
    "            \"   - Enhancing data ingestion process\"\n",
    "        )\n",
    "    \n",
    "    if results.get('answer_relevancy', 0) < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìå Low Answer Relevancy: Responses not addressing questions well. Consider:\\n\"\n",
    "            \"   - Refining system prompts\\n\"\n",
    "            \"   - Improving query understanding\\n\"\n",
    "            \"   - Adding query expansion/reformulation\"\n",
    "        )\n",
    "    \n",
    "    if retrieval_metrics['precision_at_1'] < 0.7:\n",
    "        recommendations.append(\n",
    "            \"üìå Low Retrieval Accuracy: Not finding correct tickets. Consider:\\n\"\n",
    "            \"   - Fine-tuning embedding model on domain data\\n\"\n",
    "            \"   - Improving ticket descriptions and metadata\\n\"\n",
    "            \"   - Using hybrid search (vector + keyword)\"\n",
    "        )\n",
    "    \n",
    "    if recommendations:\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"\\n{i}. {rec}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ System is performing well across all metrics!\")\n",
    "        print(\"\\nSuggested next steps:\")\n",
    "        print(\"  - Monitor performance over time\")\n",
    "        print(\"  - Test with more diverse queries\")\n",
    "        print(\"  - Evaluate on edge cases\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a7529",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides comprehensive evaluation of your RAG-based support ticket system using RAGAS metrics and custom retrieval accuracy measures. Regular evaluation helps identify areas for improvement and track system performance over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
